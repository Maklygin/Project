{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train_loop.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gNdkE8OHguhv",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import BertModel, BertTokenizer, AdamW\n",
        "from model_script import BertForSequenceClassification\n",
        "\n",
        "from torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler \n",
        "\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import matthews_corrcoef, f1_score, precision_recall_fscore_support\n",
        "\n",
        "from model_script import convert_tsv_to_model_input, convert_list_to_torch"
      ],
      "metadata": {
        "id": "q8_7mwLOq1r7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_labels = 6\n",
        "max_seq_length = 128\n",
        "batch_size = 32\n",
        "num_epoch = 3\n",
        "lr = 3e-5\n",
        "val_batch_size = 64\n",
        "\n",
        "label_map = {'false': 0, 'CPR:3': 1, 'CPR:4': 2, 'CPR:5': 3, 'CPR:6': 4, 'CPR:9': 5}\n",
        "reverse_map = {0:'false', 1:'CPR:3', 2:'CPR:4', 3:'CPR:5', 4:'CPR:6', 5:'CPR:9'}\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-uncased')"
      ],
      "metadata": {
        "id": "etcUFrzDq92P",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=lr)\n",
        "model.train()\n",
        "model.to(device)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "h4XtiuCi8XWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval(dataloader,all_label_ids,rev_dict):\n",
        "  eval_loss = 0\n",
        "  preds = []\n",
        "\n",
        "  model.eval()\n",
        "  print('###  EVALUATION  ###')\n",
        "  for step,batch in enumerate(tqdm(dataloader)):\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "    title_ids, title_mask, title_segment, input_ids, input_mask, segment_ids, \\\n",
        "      P_gauss1_list, P_gauss2_list, label_ids = batch\n",
        "\n",
        "    with torch.no_grad():\n",
        "      logs = model(title_ids, title_segment, title_mask, input_ids, segment_ids, \\\n",
        "                    input_mask, P_gauss1_list, P_gauss2_list, labels=None)\n",
        "      loss_fct = CrossEntropyLoss()\n",
        "      loss = loss_fct(logs.view(-1, num_labels), label_ids.view(-1))\n",
        "\n",
        "      eval_loss+=loss.item()\n",
        "      for i in range(len(logs.detach().cpu().numpy())):\n",
        "        preds.append(logs.detach().cpu().numpy()[i])\n",
        "    \n",
        "  eval_loss = eval_loss/len(dataloader)\n",
        "\n",
        "  preds = np.array(preds)\n",
        "  preds = preds.argmax(axis=1)\n",
        "\n",
        "  s = precision_recall_fscore_support(y_pred=[reverse_map[i] for i in preds], \n",
        "                                      y_true=[reverse_map[i] for i in all_label_ids.numpy()], \n",
        "                                      labels=[reverse_map[i] for i in range(6)], average=\"micro\")\n",
        " \n",
        "  matr = metrics.confusion_matrix(all_label_ids.numpy(), preds,labels=[0,1,2,3,4,5])\n",
        "\n",
        "  report = metrics.classification_report([reverse_map[i] for i in all_label_ids.numpy()], \n",
        "                                      [reverse_map[i] for i in preds],\n",
        "                                      labels=[reverse_map[i] for i in range(6)],\n",
        "                                      output_dict=True)\n",
        "\n",
        "\n",
        "  return(s,matr,report,eval_loss)"
      ],
      "metadata": {
        "id": "6YgVQoNj-_Q1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_input_list = convert_tsv_to_model_input('train_en.tsv', tokenizer=tokenizer, max_seq_length=max_seq_length)\n",
        "train_data = TensorDataset(*convert_list_to_torch(model_input_list)) # * for turple unboxing \n",
        "en_train_dataloader = DataLoader(train_data, sampler=RandomSampler(train_data), batch_size=batch_size)\n",
        "# Выводит количество брака в процентном соотношении от обрабатываемого датасета\n",
        "\n",
        "model_input_list = convert_tsv_to_model_input('train_ru.tsv', tokenizer=tokenizer, max_seq_length=max_seq_length)\n",
        "train_data = TensorDataset(*convert_list_to_torch(model_input_list)) # * for turple unboxing \n",
        "ru_train_dataloader = DataLoader(train_data, sampler=RandomSampler(train_data), batch_size=batch_size)\n",
        "\n",
        "en_val_input_list = convert_tsv_to_model_input('dev_en.tsv', tokenizer=tokenizer, max_seq_length=max_seq_length)\n",
        "en_val_data = TensorDataset(*convert_list_to_torch(en_val_input_list)) # * for turple unboxing \n",
        "en_val_dataloader = DataLoader(en_val_data, sampler=SequentialSampler(en_val_data), batch_size=val_batch_size)\n",
        "\n",
        "ru_val_input_list = convert_tsv_to_model_input('dev_ru.tsv', tokenizer=tokenizer, max_seq_length=max_seq_length)\n",
        "ru_val_data = TensorDataset(*convert_list_to_torch(ru_val_input_list)) # * for turple unboxing \n",
        "ru_val_dataloader = DataLoader(ru_val_data, sampler=SequentialSampler(ru_val_data), batch_size=val_batch_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "OmGFlLkI3Ebn",
        "outputId": "bdea3e69-7b2a-40bc-824e-397727a10744"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.03779729022503925\n",
            "0.11069674933184163\n",
            "0.029879740980573544\n",
            "0.1303752931978108\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('### TRAINING  ###')\n",
        "for i in range(num_epoch):\n",
        "  print('### TRAINING '+'n_epoch: ' + str(i+1) +' ###')\n",
        "  for step,batch_en in tqdm(enumerate(en_train_dataloader)):\n",
        "    batch_en = tuple(t.to(device) for t in batch_en)\n",
        "\n",
        "    if step%2 == 0:\n",
        "      optimizer.zero_grad()\n",
        "    title_ids, title_mask, title_segment, input_ids, input_mask, \\\n",
        "      segment_ids, P_gauss1_list, P_gauss2_list, label_ids = batch_en\n",
        "\n",
        "    logs = model(title_ids, title_segment, title_mask, input_ids, segment_ids, input_mask, P_gauss1_list, P_gauss2_list, labels=None)\n",
        "    loss_fct = CrossEntropyLoss()\n",
        "    loss = loss_fct(logs.view(-1, num_labels), label_ids.view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  if i != num_epoch-1:\n",
        "    _,_,_,l = eval(en_val_dataloader,convert_list_to_torch(en_val_input_list)[8],reverse_map)\n",
        "    print('###  EN Loss: ',l)\n",
        "  model.train()\n",
        "\n",
        "  for step,batch_ru in tqdm(enumerate(ru_train_dataloader)):\n",
        "    batch_ru = tuple(t.to(device) for t in batch_ru)\n",
        "\n",
        "    if step%2 == 0:\n",
        "      optimizer.zero_grad()\n",
        "    title_ids, title_mask, title_segment, input_ids, input_mask, \\\n",
        "      segment_ids, P_gauss1_list, P_gauss2_list, label_ids = batch_ru\n",
        "\n",
        "    logs = model(title_ids, title_segment, title_mask, input_ids, segment_ids, input_mask, P_gauss1_list, P_gauss2_list, labels=None)\n",
        "    loss_fct = CrossEntropyLoss()\n",
        "    loss = loss_fct(logs.view(-1, num_labels), label_ids.view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  if i != num_epoch-1:\n",
        "    _,_,_,l = eval(ru_val_dataloader,convert_list_to_torch(ru_val_input_list)[8],reverse_map)\n",
        "    print('###  RU Loss: ',l)\n",
        "  model.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a0gTVQ8wclp",
        "outputId": "35c1e748-5f1d-43ba-d375-cf1f0ec99455"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### TRAINING  ###\n",
            "### TRAINING n_epoch: 1 ###\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "518it [12:37,  1.46s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "###  EVALUATION  ###\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 164/164 [02:45<00:00,  1.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "###  EN Loss:  0.40857985587336304\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "448it [10:59,  1.47s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "###  EVALUATION  ###\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 140/140 [02:20<00:00,  1.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "###  RU Loss:  0.59231960943767\n",
            "### TRAINING n_epoch: 2 ###\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "518it [12:42,  1.47s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "###  EVALUATION  ###\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 164/164 [02:46<00:00,  1.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "###  EN Loss:  0.368140086324363\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "448it [10:59,  1.47s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "###  EVALUATION  ###\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 140/140 [02:20<00:00,  1.00s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "###  RU Loss:  0.5045649231411516\n",
            "### TRAINING n_epoch: 3 ###\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "518it [12:42,  1.47s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "###  EVALUATION  ###\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 164/164 [02:46<00:00,  1.01s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "###  EN Loss:  0.3524217285902989\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "448it [10:59,  1.47s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "###  EVALUATION  ###\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 140/140 [02:21<00:00,  1.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "###  RU Loss:  0.5266786667252226\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "torch.save(model.state_dict(), 'mBert_fine_tuned.pth')\n",
        "s,matr,report,eval_loss = eval(ru_val_dataloader,convert_list_to_torch(ru_val_input_list)[8],reverse_map)"
      ],
      "metadata": {
        "id": "Sq-BW19qQC4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"mBert_fine_tuned.pth\")"
      ],
      "metadata": {
        "id": "caaG_ElTF4qS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(train_dataloader)\n",
        "batch = tuple(t.to(device) for t in batch)\n",
        "# model.to(device)\n",
        "\n",
        "title_ids, title_mask, title_segment, input_ids, \\\n",
        "input_mask, segment_ids, P_gauss1_list, P_gauss2_list, label_ids = batch\n",
        "\n",
        "with torch.no_grad():\n",
        "  logs = model(title_ids, title_segment, title_mask, input_ids, segment_ids, \\\n",
        "                 input_mask, P_gauss1_list, P_gauss2_list, labels=None)\n",
        "  loss_fct = CrossEntropyLoss()\n",
        "  loss = loss_fct(logs.view(-1, num_labels), label_ids.view(-1))\n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "2TeyVQURzCio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# p,r,f,s = precision_recall_fscore_support(y_pred=preds, y_true=labels, \n",
        "                                          # labels=[0,1,2,3,4,5], average=\"micro\")\n",
        "# print({\"P\": p, \"R\": r, \"F1\": f,})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIaAIRS00lwH",
        "outputId": "0837b8c4-3e02-4db3-fcb5-b57bbc9b975f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 2, 2, 4, 4, 2, 2, 2, 2, 4, 4, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.save(model.state_dict(), 'model_path')\n",
        "\n",
        "# model.load_state_dict(torch.load('model_path'))\n",
        "# model.eval()"
      ],
      "metadata": {
        "id": "V5D3suSy2N6w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}